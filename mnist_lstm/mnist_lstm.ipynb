{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Tutorial\n",
    "\n",
    "In this notebook we will train a simple LSTM network in TensorFlow to classify digits from famous MNIST dataset (http://yann.lecun.com/exdb/mnist/).\n",
    "The code is based on the work of Aymeric Damien (https://github.com/aymericdamien/TensorFlow-Examples).\n",
    "\n",
    "There will be blanks left all over the code for you to fill out. If you ever get stuck, there's also a complete version in the repository.\n",
    "\n",
    "Ready? **Let's go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNIST dataset\n",
    "\n",
    "Fortunately, TensorFlow can download MNIST dataset for us automatically. It is also already divided into training, validation and test sets. We can also specify right away to transform the labels to one-hot form, which is needed for a softmax classifier. \n",
    "\n",
    "If you're not familiar with one-hot representation, read this great Quora answer (https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science). \n",
    "\n",
    "\n",
    "The process may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check training and test set\n",
    "\n",
    "It is always a good idea to check the data we work with.\n",
    "\n",
    "#### Number of train/test examples\n",
    "\n",
    "Let's start with checking the number of examples we have available, both in training and test sets (we will not be using validation set in this tutorial). To do this, explore the ```mnist``` object.\n",
    "\n",
    "_(If you're not familiar with jupyter notebooks, you can do this by typing ```mnist.``` and pressing **Tab** to see the member fields and methods.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### [TASK] Get number of train/test examples\n",
    "train_num_examples = None\n",
    "test_num_examples = None\n",
    "###\n",
    "print('Number of train examples: {0}'.format(train_num_examples))\n",
    "print('Number of train examples: {0}'.format(test_num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did this correctly, you should get below values:\n",
    "\n",
    "Name | Value\n",
    "--- | --- \n",
    "Number of train examples     | 55000 \n",
    "Number of test examples      | 10000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image shape\n",
    "\n",
    "So we know how many samples we have, but what about a single image?\n",
    "\n",
    "Let's start by getting a single example and checking its shape. It doesn't matter if it comes from train or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### [TASK] Get a random example from MNIST dataset\n",
    "mnist_example = None\n",
    "###\n",
    "\n",
    "print('Single example shape: {0}'.format(mnist_example.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "Single example shape     | (784,) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector to matrix\n",
    "\n",
    "As you can see, the image is in a vector form. It's a common practice to reshape images to this form, since it's easier to process them like this on a computer.\n",
    "\n",
    "That being said, we'd like to see what it looks like and for this we need to get the image to its original form. We have to use the knowledge of the MNIST dataset to achieve this: each digit is a square, black-and-white image (1 channel, not 3 as with normal images).\n",
    "\n",
    "With that knowledge in mind, lets use the reshape function to change ```mnist_example``` to a square matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### [TASK] Reshape mnist_example to square matrix.\n",
    "mnist_example = None\n",
    "###\n",
    "\n",
    "print('MNIST digit shape: {0}'.format(mnist_example.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "MNIST digit shape     | (28, 28) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show a random MNIST training image\n",
    "\n",
    "Now that we know how to restore digits to the original form, let's write a simple function to sample a random example from training set and show it using matplotlib. Your job is to retrive a correct label for the randomly chosen image (watch out for a one-hot vector!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_random_mnist_train_example(mnist):\n",
    "    \"\"\"Draws a random training image from MNIST dataset and displays it.\n",
    "    \n",
    "    Args:\n",
    "        mnist: MNIST dataset.\n",
    "    \"\"\"\n",
    "    random_idx = random.randint(0, mnist.train.num_examples)\n",
    "    image = mnist.train.images[random_idx].reshape(28, 28)\n",
    "    imgplot = plt.imshow(image, cmap='Greys')\n",
    "    ### [TASK] Get a correct label for the image \n",
    "    label = None\n",
    "    ###\n",
    "    print('Correct label for image #{0}: {1}'.format(random_idx, label))\n",
    "\n",
    "show_random_mnist_train_example(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "Correct label     | 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will be using a simple LSTM network with `timesteps` inputs of size `num_input` (slices of image, see preprocessing). The output of a final unrolled cell will be then fed through a simple linear layer to get a required number of outputs (`num_classes` = 10 for MNIST). Finally we will use a softmax cross-entropy loss to fit our model.\n",
    "\n",
    "Here's a picture representing our model on 5x5 image.\n",
    "\n",
    "![model_mnist_lstm](img/model_mnist_lstm.png)\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "Since RNN requires a sequence of inputs and we have only one input (image), we have to divide the image into smaller parts. We will do this by moving a small window of arbitrary size (kernel) through the image. For each position of the kernel, we take the corresponding patch of the image, reshape it to vector (for training ease) and append in to the list of kernels.\n",
    "\n",
    "We define 2 functions to do this:<br>\n",
    "    1) **get_kernels** - retrieves all patches from image using a kernel of shape `kernel_shape`, which is moved by `stride` pixels in x and y direction at each step; if a kernel is partially outside of the image, zero-padding is used <br>\n",
    "    2) **get_batch_kernels** - does the same as get_kernels, but for the batch of vector-shaped images\n",
    "    \n",
    "Your task is to write get_batch_kernels function, using get_kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_patches(image, kernel_shape, stride):\n",
    "    \"\"\"Get all patches from image using a moving kernel.\n",
    "    \n",
    "    Args:\n",
    "        image (matrix): Matrix of arbitrary shape.\n",
    "        kernel_shape (tuple): The shape of the kernel.\n",
    "        stride (tuple): Number of units to move the kernel in x and y.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Array of vector-shaped patches.\n",
    "    \"\"\"\n",
    "    image_h, image_w = image.shape\n",
    "    kernel_h, kernel_w = kernel_shape\n",
    "    stride_h, stride_w = stride\n",
    "    h = w = 0\n",
    "    patches = []\n",
    "    while h < image_h:\n",
    "        w = 0\n",
    "        while w < image_w:\n",
    "            patch = image[h:(h + kernel_h), w:(w + kernel_w)]\n",
    "            if patch.shape != (kernel_h, kernel_w):\n",
    "                # use zero padding\n",
    "                patch = np.pad(patch, ((0, kernel_h - patch.shape[0]), \n",
    "                                         (0, kernel_w - patch.shape[1])), 'constant')\n",
    "            patches.append(patch.reshape(-1))\n",
    "            w += stride_w\n",
    "        h += stride_h\n",
    "    return np.array(patches)\n",
    "\n",
    "def get_batch_patches(batch, kernel_shape, stride, image_shape=(28, 28)):\n",
    "    \"\"\"Gets patches from a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        batch (matrix): Matrix of images, shape [batch_size, image_vector_size].\n",
    "        kernel_shape (tuple): The shape of the kernel.\n",
    "        stride (tuple): Number of units to move the kernel in x and y.\n",
    "        image_shape (tuple): Shape of a single image in matrix form.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Array of patches from the batch.\n",
    "    \"\"\"\n",
    "    batch_out = []\n",
    "    ### [TASK] Retrieve patches for every image in batch:\n",
    "    for image in batch:\n",
    "        batch_out.append(None)\n",
    "    ###\n",
    "    return np.array(batch_out)\n",
    "\n",
    "\n",
    "batch_patches = get_batch_patches(mnist.train.images[:128], kernel_shape=[4, 4], stride=[4, 4])\n",
    "print('Number of batch samples: {0}'.format(len(batch_patches)))\n",
    "print('Batch of patches shape: {0}'.format(batch_patches.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "Number of batch samples    | 128 \n",
    "Batch of patches shape     | (128, 49, 16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this notebook, we will use patches of shape (28, 1). In other words, we will divide each image into collumns, receiving 28 timesteps, each having 28 elements vector as an input. Run cell below to calculate the number of timesteps and input size. \n",
    "\n",
    "After finishing this notebook you should also try playing around with these parameters to find an optimal kernel shape and stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_shape = (28, 1)\n",
    "stride = (28, 1)\n",
    "timesteps, num_input = get_patches(mnist.train.images[0].reshape(28, 28), kernel_shape, stride).shape\n",
    "print('Timesteps: {0}'.format(timesteps))\n",
    "print('Input vector size: {0}'.format(num_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "Timesteps     | 28 \n",
    "Input vector size     | 28 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config\n",
    "It is often a good idea to put all configurable parameters in one dictionary. Run cell below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = {'num_input': num_input, 'timesteps': timesteps, 'kernel_shape': kernel_shape, 'stride': stride,\n",
    "          'num_hidden': 128, 'num_classes': 10, 'learning_rate': 0.001, 'training_steps': 10000, \n",
    "          'batch_size': 128, 'display_step': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class\n",
    "\n",
    "Let's create a class representing our model. It is able to do the following:<br>\n",
    "    1) Perform forward propagation.  \n",
    "    2) Compute loss.  \n",
    "    3) Minimize loss function (train).  \n",
    "    4) Calculate accuracy.  \n",
    "   \n",
    "Note: Methods where moved outside of the class, for the ease of testing them. Normally, they would be inside the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MnistLstmModel(object):\n",
    "    def __init__(self, config):\n",
    "        # Retrieve config parameters\n",
    "        self._config = config\n",
    "        timesteps, num_input, num_hidden, num_classes, learning_rate = config['timesteps'], \\\n",
    "            config['num_input'], config['num_hidden'], config['num_classes'], config['learning_rate']\n",
    "        \n",
    "        self.X, self.Y = get_placeholders(timesteps, num_input, num_classes)\n",
    "        self.logits = forward_propagation(self.X, timesteps, num_hidden, num_classes)\n",
    "        self.loss = compute_loss(self.logits, self.Y)\n",
    "        self.train_op = optimize(self.loss, learning_rate)\n",
    "        self.accuracy, self.predictions = predict(self.logits, self.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create placeholders\n",
    "\n",
    "Before we do anything, we must provide space for the input features and output labels. That is, tensorflow must know before computation begins, what the shape of input and output is, to allocate enough space. We can do this using tf.placeholders:\n",
    "```python\n",
    "    p = tf.placeholder(type, shape)\n",
    "```\n",
    "where type is usually set to ```tf.float32``` and shape is a list specyfing the final shape. Please note that the model will be given a batch of images at each iteration, not 1 or all of them. We do this by adding a ```None``` value to ```shape``` at the first position, e.g. for a single input of shape ```[a, b]```, batch input placeholder will be of shape ```[None, a, b]```.\n",
    "\n",
    "Please create placeholders for X and Y of MNIST dataset (type: float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_placeholders(timesteps, num_input, num_classes):\n",
    "    ### [TASK] Create placeholders for input and output of MNIST dataset\n",
    "    x = None\n",
    "    y = None\n",
    "    ###\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done, run below cell to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get_placeholders test\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X, Y = get_placeholders(config['timesteps'], config['num_input'], config['num_classes'])\n",
    "print (\"X = {0}\".format(X))\n",
    "print (\"Y = {0}\".format(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "X | Tensor(\"Placeholder:0\", shape=(?, 28, 28), dtype=float32) \n",
    "Y | Tensor(\"Placeholder_1:0\", shape=(?, 10), dtype=float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward propagation\n",
    "\n",
    "Forward pass consists of 2 steps:  \n",
    "1) LSTM forward pass.  \n",
    "2) Linear layer forward pass, converting LSTM output to ```num_classes``` vector for loss computation. Only the last output is taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(x, timesteps, num_hidden, num_classes):\n",
    "    \"\"\"Forward pass.\"\"\"\n",
    "    outputs, states = get_lstm(x, timesteps, num_hidden)\n",
    "    logits = get_linear_layer(X=outputs[-1], num_in=num_hidden, num_out=num_classes)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "\n",
    "Let's start with the LSTM part. We have to create a multi layer LSTM cell. You can do this in TF in 2 steps. <br>\n",
    "1) Get a list of cells. Cell in this context is a single LSTM layer. To do this use ```tf.contrib.rnn.BasicLSTMCell```. <br>\n",
    "2) Use the list of cells to create ```tf.contrib.MultiRNNCell```, which basically assembles cells into a single model. <br>\n",
    "\n",
    "Afterwards, we use ```tf.contrib.rnn.static_rnn``` to connect our LSTM model with input.\n",
    "\n",
    "Note: ```tf.contrib.rnn.static_rnn``` requires input to be in time-major form, i.e. instead of a tensor of shape ```[batch_size, timesteps, num_input]```, we have to get _timesteps_ number of tensors of shape ```[batch_size, num_input]```. We can do this by using ```tf.unstack```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lstm(x, timesteps, num_hidden, num_layers=1):\n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        ### [TASK] Append a basic LSTM cell.\n",
    "        cells.append(None)\n",
    "        ###\n",
    "    ### [TASK] Construct a multi RNN cell.\n",
    "    lstm_cell = None\n",
    "    ###\n",
    "    \n",
    "    ### [TASK] Unstack input x.\n",
    "    x = None\n",
    "    ###\n",
    "    \n",
    "    ### [TASK] Create static RNN of cells.\n",
    "    outputs, states = None\n",
    "    ###\n",
    "    return outputs, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done, run below cell to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### get_lstm test\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X, _ = get_placeholders(config['timesteps'], config['num_input'], config['num_classes'])\n",
    "outputs, states = get_lstm(X, config['timesteps'], config['num_hidden'])\n",
    "print(\"Outputs shape: {0}\".format(len(outputs)))\n",
    "print(\"States type: {0}\".format(type(states[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "Outputs shape | 28 \n",
    "States type   | &lt;class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear layer\n",
    "\n",
    "We need to convert outputs of last LSTM to a num_classes sized vector. To do this, we will create a linear layer of weights and biases. You can use ```tf.get_variable``` to create weights and bias tensors, specifying shape and initializer. For this example, please use ```tf.random_normal_initializer(seed=1)``` (seed=1 to get the same results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_linear_layer(X, num_in, num_out):\n",
    "    ### [TASK] Create weights and bias variables and compute X*W + b\n",
    "    W = None\n",
    "    b = None\n",
    "    output = None\n",
    "    ###\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you implemented get_linear_layer, forward_propagation should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### forward_propagation test\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (100, 200))\n",
    "output = get_linear_layer(X, 200, 10)\n",
    "print(\"Linear layer output: {0}\".format(output))\n",
    "\n",
    "# forward_propagation test\n",
    "tf.reset_default_graph()\n",
    "X, _ = get_placeholders(config['timesteps'], config['num_input'], config['num_classes'])\n",
    "logits = forward_propagation(X, config['timesteps'], config['num_hidden'], config['num_classes'])\n",
    "print(\"Logits: {0}\".format(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "Linear layer output | Tensor(\"add:0\", shape=(100, 10), dtype=float32) \n",
    "Logits              | Tensor(\"add:0\", shape=(?, 10), dtype=float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "\n",
    "Let's use ```tf.nn.softmax_cross_entropy_with_logits``` as our loss function. Please remember to return the mean value of the loss across the whole batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels):\n",
    "    ### [TASK] Calculate softmax cross-entropy loss (cost).\n",
    "    loss = None\n",
    "    ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done, run below cell to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute_loss test\n",
    "tf.reset_default_graph()\n",
    "X, Y = get_placeholders(config['timesteps'], config['num_input'], config['num_classes'])\n",
    "logits = forward_propagation(X, config['timesteps'], config['num_hidden'], config['num_classes'])\n",
    "loss = compute_loss(logits, Y)\n",
    "print(\"Loss: {0}\".format(loss))\n",
    "print(\"Loss shape: {0}\".format(loss.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "---  | --- \n",
    "Loss       | Tensor(\"Mean:0\", shape=(), dtype=float32) \n",
    "Loss shape | () "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "Let's minimize our loss function with ```tf.train.GradientDescentOptimizer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(loss, learning_rate):\n",
    "    ### [TASK] Minimize loss function using SGD.\n",
    "    train_op = None\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done, run below cell to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimize test\n",
    "tf.reset_default_graph()\n",
    "X, Y = get_placeholders(config['timesteps'], config['num_input'], config['num_classes'])\n",
    "logits = forward_propagation(X, config['timesteps'], config['num_hidden'], config['num_classes'])\n",
    "loss = compute_loss(logits, Y)\n",
    "train_op = optimize(loss, config['learning_rate'])\n",
    "print(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "---  | --- \n",
    "name | \"GradientDescent\"\n",
    "op | \"NoOp\"\n",
    "input | \"^GradientDescent/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/ApplyGradientDescent\"\n",
    "input | \"^GradientDescent/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/ApplyGradientDescent\"\n",
    "input | \"^GradientDescent/update_weights_out/ApplyGradientDescent\"\n",
    "input | \"^GradientDescent/update_bias_out/ApplyGradientDescent\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "We want to know how well our model is performing in human-readable form. We will use a simple accuracy metric for this purpose:  \n",
    "1) Calculate softmax with ```tf.nn.softmax```  \n",
    "2) Get predicted (most probable) labels. (_hint: use ```tf.argmax```_)  \n",
    "3) Get a vector of correct predictions: if correct_pred[i] = 1, model predicted label correctly. (_hint: use ```tf.equal```_)  \n",
    "4) Get a mean value of above to calculate accuracy (_hint: you may need to convert tensorf to ```tf.float32``` first_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(logits, labels):\n",
    "    # Convert labels to numerical values instead of one-hot vector\n",
    "    correct_labels = tf.argmax(labels, 1)\n",
    "    ### [TASK] Calcuate prediction accuracy\n",
    "    y_pred = None  # 1) Calculate softmax.\n",
    "    predictions = None  # 2) Get predicted labels.\n",
    "    correct_pred = None  # 3) Check if predictions are correct.\n",
    "    accuracy = None  # 4) Average across correct predictions.\n",
    "    ###\n",
    "    return accuracy, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you're done, run below cell to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict test\n",
    "tf.reset_default_graph()\n",
    "X, Y = get_placeholders(config['timesteps'], config['num_input'], config['num_classes'])\n",
    "logits = forward_propagation(X, config['timesteps'], config['num_hidden'], config['num_classes'])\n",
    "accuracy, predictions = predict(logits, Y)\n",
    "print(\"Accuracy: {0}\".format(accuracy))\n",
    "print(\"Predictions: {0}\".format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "---  | --- \n",
    "Accuracy | Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
    "Predictions | Tensor(\"ArgMax_1:0\", shape=(?,), dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Let's train our model and see how well it performs on MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "model = MnistLstmModel(config)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    log_steps = []\n",
    "    # config['training_steps'] = 200\n",
    "    for step in range(1, config['training_steps'] + 1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(config['batch_size'])\n",
    "        batch_x = get_batch_patches(batch_x, config['kernel_shape'], config['stride'])\n",
    "        sess.run(model.train_op, feed_dict={model.X: batch_x, model.Y: batch_y})\n",
    "        if step % config['display_step'] == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([model.loss, model.accuracy], feed_dict={model.X: batch_x,\n",
    "                                                                          model.Y: batch_y})\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "            log_steps.append(step)\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # plot the loss\n",
    "    plt.plot(log_steps, losses)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # plot the accuracy\n",
    "    plt.plot(log_steps, accs)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title('Train accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_images = mnist.test.images[:test_len]\n",
    "    test_data = get_batch_patches(test_images, config['kernel_shape'], config['stride'])\n",
    "    test_labels = mnist.test.labels[:test_len]\n",
    "    acc, predictions = sess.run([model.accuracy, model.predictions], \n",
    "                                 feed_dict={model.X: test_data, model.Y: test_labels})\n",
    "    print(\"Testing Accuracy: {0}\".format(acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Value\n",
    "--- | --- \n",
    "Step 1 | Minibatch Loss= 3.1658, Training Accuracy= 0.133 \n",
    "Step 200 | Minibatch Loss= 2.1227, Training Accuracy= 0.367 \n",
    "... | ... \n",
    "Step 10000 | Minibatch Loss= 0.2749, Training Accuracy= 0.922 \n",
    "Testing Accuracy | 0.96875 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where our model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correct_labels = np.argmax(test_labels, 1)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] != correct_labels[i]:\n",
    "        print(\"Prediction: {0}, correct label: {1}\".format(predictions[i], correct_labels[i]))\n",
    "        image = test_images[i].reshape(28, 28)\n",
    "        imgplot = plt.imshow(image, cmap='Greys')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You finished the assignment. Feel free to play around and experiment with hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
